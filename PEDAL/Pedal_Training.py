__author__ = 'Vince'
import sys
import os
import logging
from itertools import cycle
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import Normalizer
from sklearn.svm import SVC
from scipy.sparse import *
from scipy import *
import scipy.sparse as ssp
from sklearn.feature_selection import SelectKBest, chi2, f_classif
from collections import OrderedDict
from sklearn.cross_validation import train_test_split
from random import randint
from random import shuffle
from sklearn import metrics
from sklearn.externals import joblib
from time import time
from scipy.sparse import vstack

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger('sys.stdout')

def GetFilesWithExtn (RootDir, Extn):
    '''
    List all the files from the RootDir with a given Extn
    :param RootDir: root to find files from
    :param Extn: extension to look for
    :return: sorted list of files with a given extension from the root dir and its child dirs
    '''
    FilesToProcess = []
    for Root, Folders, Files in os.walk(RootDir):
        for F in Files:
            if F.endswith(Extn):
                FilesToProcess.append(os.path.join(Root, F))
    FilesToProcess = list(set(FilesToProcess))
    FilesToProcess.sort()
    return FilesToProcess

def NewLineTokenizer (Str):
    '''
    New line tokenizer
    :param Str:
    :return:
    '''
    return Str.split('\n')

def SelectiveTokenizer(Str):
    '''
    Selective tokenizer based on feature combination
    :param Str:
    :return:
    '''
    global FeatureCombination
    a = Str.split('\n')
    if not int(FeatureCombination[0]):
        for elem in a:
            if elem.startswith('comp_'):
                del a[a.index(elem)]
    if not int(FeatureCombination[1]):
        for elem in a:
            if '.permission.' in elem:
                del a[a.index(elem)]
    if not int(FeatureCombination[2]):
        for elem in a:
            if elem.startswith('vi'):
                del a[a.index(elem)]
    if not int(FeatureCombination[3]):
        for elem in a:
            if (elem.startswith('src') or elem.startswith('sink')):
                del a[a.index(elem)]
    if not int(FeatureCombination[4]):
        for elem in a:
            if (elem.endswith('rtpermcheck_yes') or elem.endswith('rtpermcheck_no')):
                del a[a.index(elem)]
    if not int(FeatureCombination[5]):
        for elem in a:
            if (elem.startswith('kw') or elem.startswith('stemmedkw')):
                del a[a.index(elem)]
    return a

def Classification(MalwareCorpus,
                   GoodwareCorpus,
                   Extn):
    '''
    Classification
    :param MalwareCorpus: Absolute path of Ad lib feature files
    :param GoodwareCorpus: Absolute path of Non-ad lib feature files
    :param Extn: Extension of feature files
    :return:
    '''

    # step 1 - split all samples to training set and test set
    logger.debug("Loading positive and negative samples file basename")

    MalSamples = GetFilesWithExtn(MalwareCorpus, Extn)
    GoodSamples = GetFilesWithExtn(GoodwareCorpus, Extn)

    logger.info("All Samples loaded")
    print '# Ad samples:', len(MalSamples)
    print '# Non-Ad samples:', len(GoodSamples)

    # step 2 - split the dataset into training and testing sets
    TrainMalSamples, TestMalSamples = train_test_split(MalSamples, test_size=0.3, random_state=randint(0,99))
    TrainGoodSamples, TestGoodSamples = train_test_split(GoodSamples, test_size=0.3, random_state=randint(0,99))
    logger.info("Training and test sets split randomly")


    TrainMalLabels = np.ones(len(TrainMalSamples)).tolist()
    TestMalLabels = np.ones(len(TestMalSamples)).tolist()
    TrainGoodLabels = np.empty(len(TrainGoodSamples)); TrainGoodLabels.fill(-1); TrainGoodLabels = TrainGoodLabels.tolist()
    TestGoodLabels = np.ones(len(TestGoodSamples)); TestGoodLabels.fill(-1); TestGoodLabels = TestGoodLabels.tolist()
    logger.info("All labels created")

    TrainSamples = TrainMalSamples + TrainGoodSamples
    TestSamples = TestMalSamples + TestGoodSamples
    TrainLabels = TrainMalLabels + TrainGoodLabels
    TestLabels = TestMalLabels + TestGoodLabels

    del MalSamples, GoodSamples
    logger.info("All Samples loaded into training and testing sets")
    print "# Train Samples", len(TrainSamples)
    print "# Train Labels", len(TrainLabels)
    print "# Test Samples", len(TestSamples)
    print "# Test Labels", len(TestLabels)

    # step 3 - feature extracting
    normalizer = Normalizer()
    NewLineCVetorizer = CountVectorizer(input=u'filename',
                                        lowercase=True,
                                        token_pattern=None,
                                        binary=True,
                                        tokenizer=SelectiveTokenizer,
                                        stop_words=['internetaccesscheck_yes','internetaccesscheck_no'],
                                        dtype=np.float64)

    print 'performing count vectorizing'
    TrainDocsTermsFVs = NewLineCVetorizer.fit_transform(TrainSamples)
    TestDocsTermsFVs = NewLineCVetorizer.transform(TestSamples)

    TrainFVs = normalizer.fit_transform(TrainDocsTermsFVs)
    TestFVs = normalizer.transform(TestDocsTermsFVs)

    print 'train term-doc matrix: ', TrainFVs.shape 
    print 'test term-doc matrix: ', TestFVs.shape 

    # step 4 - training classifier
    f = open('TrainingReport.txt', 'w')
    Clf = SVC(kernel='sigmoid', gamma=0.125)
    BestModel = Clf.fit(TrainFVs, TrainLabels)

    PredictedLebals = BestModel.predict(TestFVs)

    print >>f, "Test Set Accuracy = ", metrics.accuracy_score(TestLabels, PredictedLebals)
    print >>f, (metrics.classification_report(TestLabels, PredictedLebals, target_names=['Ad_Lib', 'Non-Ad_Lib']))
    print >>f, "Accuracy classification score:", metrics.accuracy_score(TestLabels, PredictedLebals)
    print >>f, "Hamming loss:", metrics.hamming_loss(TestLabels, PredictedLebals)
    print >>f, "Average hinge loss:", metrics.hinge_loss(TestLabels, PredictedLebals)
    print >>f, "Log loss:", metrics.log_loss(TestLabels, PredictedLebals)
    print >>f, "F1 Score:", metrics.f1_score(TestLabels, PredictedLebals)
    print >>f, "Zero-one classification loss:", metrics.zero_one_loss(TestLabels, PredictedLebals)
    print >>f, '-'*100

    WholeFVs = vstack([TrainFVs, TestFVs])
    WholeLabels = TrainLabels + TestLabels
    BestModel = Clf.fit(WholeFVs, WholeLabels)

    '''
    # step 5 - dumping top features
    print >>f, 'Top features'
    print >>f, '-'*100
    Vocab = NewLineCVetorizer.get_feature_names()
    FeautureImportances = BestModel.coef_[0]
    TopFeatureIndices = FeautureImportances.argsort()[-100:][::-1]
    for FIndex in TopFeatureIndices:
        print >>f, Vocab[FIndex], FeautureImportances[FIndex]
    print >>f, '-'*100

    for i in xrange(len(TestSamples)):
        if -1 == TestLabels[i]:
            TestFVss= TestFVs[:i, :]
    print >>f, 'Possitive Top Features'

    FeatureImportancesSparseArray = ssp.lil_matrix((TestFVss.shape[1], TestFVss.shape[1]))
    FeatureImportancesSparseArray.setdiag(FeautureImportances)
    AllFVsTimesW = TestFVss*FeatureImportancesSparseArray

    AvgFVOfDay = AllFVsTimesW.mean(axis=0)
    AvgFVOfDay = AvgFVOfDay.view(dtype=np.float64).reshape(AvgFVOfDay.shape[1],-1)
    AvgFVOfDay = np.array(AvgFVOfDay).reshape(-1,)
    TopRes = AvgFVOfDay.argsort()[-100:][::-1]
    for Sindex in TopRes:
            print >>f, Vocab[Sindex], AvgFVOfDay[Sindex]
    print >>f, '-'*100

    for i in xrange(len(TestSamples)):
        if -1 == TestLabels[i]:
            TestFVsss= TestFVs[i:, :]
    print >>f, 'Negative Top Features'

    FeatureImportancesSparseArray = ssp.lil_matrix((TestFVss.shape[1],TestFVss.shape[1]))
    FeatureImportancesSparseArray.setdiag(FeautureImportances)
    AllFVsTimesW = TestFVsss*FeatureImportancesSparseArray

    AvgFVOfDay = AllFVsTimesW.mean(axis=0)
    AvgFVOfDay = AvgFVOfDay.view(dtype=np.float64).reshape(AvgFVOfDay.shape[1],-1)
    AvgFVOfDay = np.array(AvgFVOfDay).reshape(-1,)
    TopRes = AvgFVOfDay.argsort()[:100][::-1]
    for Sindex in TopRes:
            print >>f, Vocab[Sindex], AvgFVOfDay[Sindex]
    print >>f, '-'*100    
    '''
    # step 6 - dumping model
    Vocab = NewLineCVetorizer.get_feature_names()
    with open('Vocab.txt', 'w') as f_w:
        for word in Vocab:
            print >>f_w, word

    joblib.dump(BestModel, 'Model.pkl')


def main(MalwareDirName, GoodwareDirName, Extn):

    Classification(MalwareDirName, GoodwareDirName, Extn)

if __name__ == "__main__":

    FeatureCombination = sys.argv[4]
    main(MalwareDirName=sys.argv[1],
         GoodwareDirName=sys.argv[2],
         Extn=sys.argv[3])
