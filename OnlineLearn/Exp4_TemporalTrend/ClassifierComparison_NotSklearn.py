__author__ = 'Vince'
import sys
import os
import logging
from itertools import cycle
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.preprocessing import Normalizer
from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.linear_model import Perceptron
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics
from sklearn import grid_search
from time import time
from sklearn.svm import LinearSVC
from scipy.sparse import *
from scipy import *
import scipy.sparse as ssp
from sklearn.feature_selection import SelectKBest, chi2, f_classif
from collections import OrderedDict
from sklearn.cross_validation import train_test_split
from random import randint
from random import shuffle
import multiprocessing as mp
import multiprocessing.pool

from scipy.sparse import vstack
sys.path.append('../arow')
sys.path.append('arow')
sys.path.append('../cw')
sys.path.append('cw')

from arow_diag import ArowDiag
from multiclass_confidence_weighted_var_diag import MCWVarDiag

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger('sys.stdout')

def GetFilesWithExtn (RootDir, Extn):
    '''
    lllr to "find" command. List all the files from the RootDir with a given Extn
    :param RootDir: root to find files from
    :param Extn: extension to look for
    :return: sorted list of files with a given extension from the root dir and its child dirs
    '''

    FilesToProcess = []
    for Root, Folders, Files in os.walk(RootDir):
        for F in Files:
            if F.endswith(Extn):
                FilesToProcess.append(os.path.join(Root,F))

    FilesToProcess = list(set(FilesToProcess))
    FilesToProcess.sort()
    return FilesToProcess


def NewLineTokenizer (Str):
    return Str.split('\n')


def GetNonZeroDims (NonZeroDims, TrainFVs):
    ColSums = TrainFVs.sum(axis = 0)
    # print type(ColSums)
    ColSums = ColSums.tolist()[0]
    for CIndex, Val in enumerate(ColSums):
        if Val:
            NonZeroDims.add (CIndex)
    # print NonZeroDims
    return NonZeroDims


def ReadVocab (VocabFName):
    '''
    Read vocabulary
    :param VocabFName: vocabulary file
    :return:
    '''
    try:
        print 'trying to load vocab from', VocabFName
        Vocab = [l.strip() for l in open (VocabFName,'r').xreadlines()]
        return Vocab
    except:
        print 'could not load vocab from {} exiting... '.format(VocabFName)
        exit(0)


def GridSearchCV(estimator, CV, parameter, NumofSamples, FVs, Labels):
    '''
    Grid search function dedicated for cw & arow
    :param estimator: classifier class, cw or arow
    :param CV: num of CV
    :param parameter:
    :param NumofSamples: number of samples used for training
    :param FVs: Training FV
    :param Labels: True training labels
    :return: best parameter after grid search
    '''
    subpool = mp.Pool(int(CV))
    a = [subpool.apply_async(GetAccuracy, (estimator, parameter, NumofSamples, FVs, Labels,)) for i in range(int(CV))]
    Scores = [res.get() for res in a]
    subpool.close()
    subpool.join()
    return max(Scores)


def GetAccuracy(estimator, parameter, NumofSamples, FVs, Labels):
    '''
    Get accuracy score for each parameter
    :param estimator: classifier class, cw or arow
    :param parameter:
    :param NumofSamples: number of samples used for training
    :param FVs:  Training FV
    :param Labels: True training labels
    :return: accuracy score for the parameter
    '''
    try:
        BestModel = estimator(parameter, epochs=50)
    except:
        BestModel = estimator(parameter, n_iters=50)
    NumArray = range(NumofSamples)
    shuffle(NumArray)
    TrainFV = FVs[NumArray[0]]
    TrainLabel = []
    TrainLabel.append(Labels[NumArray[0]])
    for i in NumArray[:int(NumofSamples*0.7)]:
        TrainFV = ssp.vstack((TrainFV, FVs[i]), format='csr')
        TrainLabel.append(Labels[i])
    BestModel.fit(TrainFV, TrainLabel)
    PLabels = []
    TLabels = []
    for j in NumArray[int(NumofSamples*0.7):]:
        PLabel = BestModel.predict(FVs[j])
        PLabels.append(PLabel)
        TLabels.append(Labels[j])
    Accuracy = metrics.accuracy_score(PLabels, TLabels)
    return Accuracy

class NoDaemonProcess(mp.Process):
    # make 'daemon' attribute always return False
    def _get_daemon(self):
        return False
    def _set_daemon(self, value):
        pass
    daemon = property(_get_daemon, _set_daemon)

# here sub-class multiprocessing.pool.Pool is used instead of multiprocessing.Pool
# because the latter is only a wrapper function, not a proper class.
class MyPool(multiprocessing.pool.Pool):
    Process = NoDaemonProcess

def Classification(MalwareCorpus,
                   GoodwareCorpus,
                   Split,
                   Extn):
    '''
    Comparison between different online classifiers
    :param MalwareCorpus: Directory of malicious feature files
    :param GoodwareCorpus: Directory of benign feature files
    :param Split: test set split
    :param Extn: Extension of feature files
    :return:
    '''
    if 'datatxt' in Extn:
        Type = 'Drebin'
    elif 'WL2' in Extn:
        Type = 'WLK'
    elif '_pkg_adicfg_ret_.json.ADG.DirWLWODup' in Extn:
        Type = 'CWLK'
    else:
        Type = 'Other'


    # step 1 - split all samples to training set and test set
    logger.debug ("Loading positive and negative samples file basename")

    MalSamples = GetFilesWithExtn(MalwareCorpus,Extn)
    GoodSamples = GetFilesWithExtn(GoodwareCorpus,Extn)[:len(MalSamples)]

    logger.info ("All Samples loaded")
    print '# mal samples:', len(MalSamples)
    print '# good samples:', len(GoodSamples)


    TrainMalSamples = MalSamples[:int(len(MalSamples)*(1-Split))]
    TestMalSamples = MalSamples[int(len(MalSamples)*(1-Split)):]

    TrainGoodSamples = GoodSamples[:int(len(GoodSamples)*(1-Split))]
    TestGoodSamples = GoodSamples[int(len(GoodSamples)*(1-Split)):]



    logger.info("Training and test sets split finished")

    TrainMalLabels = np.ones(len(TrainMalSamples)).tolist()
    TestMalLabels = np.ones(len(TestMalSamples)).tolist()
    TrainGoodLabels = np.empty(len(TrainGoodSamples));TrainGoodLabels.fill(-1); TrainGoodLabels = TrainGoodLabels.tolist()
    TestGoodLabels = np.ones(len(TestGoodSamples));TestGoodLabels.fill(-1); TestGoodLabels = TestGoodLabels.tolist()
    logger.info ("All labels created")

    TrainSamples = TrainMalSamples + TrainGoodSamples
    TestSamples = TestMalSamples + TestGoodSamples
    TrainLabels = TrainMalLabels + TrainGoodLabels
    TestLabels = TestMalLabels + TestGoodLabels
    NumTestMalSamples = len(TestMalLabels)

    del MalSamples, GoodSamples
    logger.info ("All Samples loaded into training and testing sets")
    print "# Train Samples", len(TrainSamples)
    print "# Train Labels", len(TrainLabels)
    print "# Test Samples", len(TestSamples)
    print "# Test Labels", len(TestLabels)

    #step 2 - feature extracting
    TFIDFTransformer = TfidfTransformer()

    NewLineCVetorizer = CountVectorizer(input=u'filename',
                                                  lowercase=True,
                                                  token_pattern=None,
                                                  tokenizer=NewLineTokenizer,
                                                  dtype=np.float64)

    print 'performing count vectorizing'
    TrainDocsTermsFVs = NewLineCVetorizer.fit_transform(TrainSamples)
    TestDocsTermsFVs = NewLineCVetorizer.transform(TestSamples)
    print 'performing tf-idf vectorizing'
    TrainFVs = TFIDFTransformer.fit_transform(TrainDocsTermsFVs)
    TestFVs = TFIDFTransformer.transform(TestDocsTermsFVs)


    print 'train term-doc matrix: ', TrainFVs.shape #rowsx cols, rows = docs, cols = features/terms
    print 'test term-doc matrix: ', TestFVs.shape

    #step 3- classification
    logger.info("Performing Cross Validation")

    EtaList = [ 0, 0.1, 0.3, 0.5, 0.7, 0.9, 1]
    CWAccuracyList = []

    CList = [ 0.001, 0.01, 0.1, 1, 10, 100, 1000]
    AROWAccuracyList = []


    pool = MyPool(4)

    a = [pool.apply_async(GridSearchCV, (MCWVarDiag, 5, e, len(TrainSamples), TrainFVs, TrainLabels, )) for e in EtaList]
    CWAccuracyList = [res.get() for res in a]
    EtaBest = EtaList[CWAccuracyList.index(max(CWAccuracyList))]
    BestModel_CW = MCWVarDiag(EtaBest, epochs=50)

    a = [pool.apply_async(GridSearchCV, args=(ArowDiag, 5, c, len(TrainSamples), TrainFVs, TrainLabels, )) for c in CList]
    AROWAccuracyList = [res.get() for res in a]
    CBest = CList[AROWAccuracyList.index(max(AROWAccuracyList))]
    BestModel_AROW = ArowDiag(CBest, n_iters=50)
    pool.close()
    pool.join()

    print 'best model', BestModel_CW, max(CWAccuracyList)
    print 'best model', BestModel_AROW, max(AROWAccuracyList)

    logger.info("Applying Best Model on Testing Set")

    modeldict = { BestModel_CW:'CW', BestModel_AROW:'AROW'}
    for Model in [BestModel_CW, BestModel_AROW]:
        T0 = time()
        f = open(modeldict[Model]+'_'+Type+'.txt', 'w')
        f1 = open(modeldict[Model]+'_'+Type+'_Metadata.txt', 'w')

        Model.fit(TrainFVs, TrainLabels)
        PredictedLabels = []
        NewTestLabels = []
        i = 0
        for TestFV, TestLabel in zip(TestFVs, TestLabels):
            #Mal Sample
            if i < NumTestMalSamples:
                TestMalLabel = np.array([TestLabel])
                PredictedLabel = Model.predict(TestFV)
                PredictedLabels.append(float(PredictedLabel))
                NewTestLabels.append(TestLabel)
                if float(PredictedLabel) != TestLabel:
                    try:
                        Model.partial_fit(TestFV, TestLabel)#update the model
                        logger.info("Model Partially Fitted")
                    except:
                        logger.error("Partially Fitted Failed")
                        pass
                PredictedMalLabel = np.array([float(PredictedLabel)])
                print >>f1, (metrics.classification_report(TestMalLabel,PredictedMalLabel, target_names=['Sample', 'Sample']))
                print >>f1, "Zero-one classification loss:", metrics.zero_one_loss(TestMalLabel, PredictedMalLabel)
                print >>f1, '-'*100
            #Ben Sample
            if NumTestMalSamples+i < len(TestLabels):
                TestLabel = TestLabels[NumTestMalSamples+i]
                TestFV = TestFVs[NumTestMalSamples+i]
                TestGoodLabel = np.array([TestLabel])
                PredictedLabel2 = Model.predict(TestFV)
                PredictedLabels.append(float(PredictedLabel2))
                NewTestLabels.append(TestLabel)
                if float(PredictedLabel2) != TestLabel:
                    try:
                        Model.partial_fit(TestFVs[NumTestMalSamples+i], TestLabel)#update the model
                        logger.info("Model Partially Fitted")
                    except:
                        logger.error("Partially Fitted Failed")
                        pass
                PredictedGoodLabel = np.array([float(PredictedLabel2)])
                print >>f1, (metrics.classification_report(TestGoodLabel,PredictedGoodLabel, target_names=['Sample', 'Sample']))
                print >>f1, "Zero-one classification loss:", metrics.zero_one_loss(TestGoodLabel, PredictedGoodLabel)
                print >>f1, '-'*100
            i += 1

        if modeldict[Model] == 'CW':
            print >>f, 'Best Eta parameter',EtaBest
        elif modeldict[Model] == 'AROW':
            print >>f, 'Best C parameter',CBest
        print >>f, '-'*100
        print >>f, '-'*43+'Whole Database'+'-'*43
        Accuracy = metrics.accuracy_score(PredictedLabels, NewTestLabels)
        print >>f, "Test Set Accuracy = ", Accuracy
        print >>f, 'testing time', time() - T0
        print >>f, (metrics.classification_report(NewTestLabels,
                PredictedLabels, target_names=['Goodware', 'Malware']))    # raw_input()

        print >>f, 'Classifier Top features'
        print >>f, '-'*100
        Vocab = NewLineCVetorizer.get_feature_names()
        try:
            FeautureImportances = Model.model["mu"][1.0].toarray()[0][:-1]
        except:
            FeautureImportances = Model.model["mu"].toarray()[0]
        TopFeatureIndices = FeautureImportances.argsort()[-100:][::-1]
        for FIndex in TopFeatureIndices:
            print >>f, Vocab[FIndex], FeautureImportances[FIndex]
        print >>f, '-'*100


        print >>f, 'before deleting rows TestFVs.shape', TestFVs.shape
        for i in xrange(len(TestSamples)):
            if -1 == TestLabels[i]:
                TestFVss= TestFVs[:i, :]
                break
        print >>f, 'after deleting rows TestFVs.shape', TestFVss.shape

        FeatureImportancesSparseArray = ssp.lil_matrix((TestFVss.shape[1],TestFVss.shape[1]))
        FeatureImportancesSparseArray.setdiag(FeautureImportances)
        AllFVsTimesW = TestFVss*FeatureImportancesSparseArray

        print >>f, '-'*100
        AvgFV = AllFVsTimesW.mean(axis=0)
        AvgFV = AvgFV.view(dtype=np.float64).reshape(AvgFV.shape[1],-1)
        AvgFV = np.array(AvgFV).reshape(-1,)
        TopRes = AvgFV.argsort()[-100:][::-1]
        print >>f, 'Top Feats of Test Positive Vector * Feature Importance Vector'
        for Sindex in TopRes:
            print >>f, Vocab[Sindex], AvgFV[Sindex]
        print >>f, '-'*100




def main(MalwareDirName,
         GoodwareDirName,
         TestSplit,
         Extn):

    Classification(MalwareDirName,
                   GoodwareDirName,
                   TestSplit,
                   Extn)

if __name__ == "__main__":
     main(MalwareDirName=sys.argv[1],
          GoodwareDirName=sys.argv[2],
          TestSplit=float(sys.argv[3]),
          Extn=sys.argv[4])