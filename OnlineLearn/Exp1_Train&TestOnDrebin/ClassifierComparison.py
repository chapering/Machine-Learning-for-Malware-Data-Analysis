__author__ = 'Vince'
import sys
import os
import logging
from itertools import cycle
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.preprocessing import Normalizer
from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.linear_model import Perceptron
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics
from sklearn import grid_search
from time import time
from sklearn.svm import LinearSVC
from scipy.sparse import *
from scipy import *
import scipy.sparse as ssp
from sklearn.feature_selection import SelectKBest, chi2, f_classif
from collections import OrderedDict
from sklearn.cross_validation import train_test_split
from random import randint
from random import shuffle

from scipy.sparse import vstack


logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger('sys.stdout')

def GetFilesWithExtn (RootDir, Extn):
    '''
    List all the files from the RootDir with a given Extn
    :param RootDir: root to find files from
    :param Extn: extension to look for
    :return: sorted list of files with a given extension from the root dir and its child dirs
    '''

    FilesToProcess = []
    for Root, Folders, Files in os.walk(RootDir):
        for F in Files:
            if F.endswith(Extn):
                FilesToProcess.append(os.path.join(Root,F))

    FilesToProcess = list(set(FilesToProcess))
    FilesToProcess.sort()
    return FilesToProcess

def NewLineTokenizer (Str):
    return Str.split('\n')

def ReadVocab (VocabFName):
    '''
    Read vocabulary
    :param VocabFName: vocabulary file
    :return:
    '''
    try:
        print 'trying to load vocab from', VocabFName
        Vocab = [l.strip() for l in open (VocabFName,'r').xreadlines()]
        return Vocab
    except:
        print 'could not load vocab from {} exiting... '.format(VocabFName)
        exit(0)

def Classification(MalwareCorpus,
                   GoodwareCorpus,
                   Split,
                   Extn):
    '''
    Comparison between different online classifiers
    :param MalwareCorpus: Directory of malicious feature files
    :param GoodwareCorpus: Directory of benign feature files
    :param Split: test set split
    :param Extn: Extension of feature files
    :return:
    '''
    if 'datatxt' in Extn:
        Type = 'Drebin'
    elif 'WL2' in Extn:
        Type = 'WLK'
    elif '_pkg_adicfg_ret_.json.ADG.DirWLWODup' in Extn:
        Type = 'CWLK'
    else:
        Type = 'Other'

    BestModel_SGD = grid_search.GridSearchCV(SGDClassifier(n_jobs=-1,  warm_start=True), cv=5, param_grid={'alpha':[ 0.0001, 0.001, 0.01, 1, 10, 100, 1000],
                                    'n_iter':[1, 5, 10, 50, 100],
                                    'loss':['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'], 
                                    'penalty': ['none', 'l2', 'l1', 'elasticnet']})
    BestModel_PA = grid_search.GridSearchCV(PassiveAggressiveClassifier(n_jobs=-1, warm_start=True), cv=5, param_grid={'C':[ 0.001, 0.01, 0.1, 1, 10, 100, 1000],
                                    'n_iter':[1, 5, 10, 50, 100], 'loss':['hinge', 'squared_hinge']})
    BestModel_Perceptron = grid_search.GridSearchCV(Perceptron(n_jobs=-1, warm_start=True), cv=5, param_grid={'alpha':[ 0.0001, 0.001, 0.01, 1, 10, 100, 1000],
                                    'penalty':[None, 'l2', 'l1', 'elasticnet'],
                                    'n_iter':[1, 5, 10, 50, 100]})
    BestModel_SVM = grid_search.GridSearchCV(LinearSVC(), cv=5, param_grid={'C':[ 0.001, 0.01, 0.1, 1, 10, 100, 1000],
                                    'max_iter':[1, 5, 10, 50, 100], 'loss':['hinge', 'squared_hinge']})

    # step 1 - split all samples to training set and test set
    logger.debug("Loading positive and negative samples file basename")

    MalSamples = GetFilesWithExtn(MalwareCorpus,Extn)
    GoodSamples = GetFilesWithExtn(GoodwareCorpus,Extn)[:len(MalSamples)]

    logger.info("All Samples loaded")
    print '# mal samples:', len(MalSamples)
    print '# good samples:', len (GoodSamples)

    TrainMalSamples, TestMalSamples = train_test_split(MalSamples,
            test_size=Split,
            random_state=randint(0,99))
    with open('TrainMalSamples.txt','w') as f:
        for sample in TrainMalSamples:
            print >> f, sample

    TrainGoodSamples, TestGoodSamples = train_test_split(GoodSamples,
            test_size=Split,
            random_state=randint(0,99))
    with open('TrainGoodSamples.txt','w') as f:
        for sample in TrainGoodSamples:
            print >> f, sample
    logger.info("Training and test sets split randomly")

    TrainMalLabels = np.ones(len(TrainMalSamples)).tolist()
    TestMalLabels = np.ones(len(TestMalSamples)).tolist()
    TrainGoodLabels = np.empty(len(TrainGoodSamples));TrainGoodLabels.fill(-1); TrainGoodLabels = TrainGoodLabels.tolist()
    TestGoodLabels = np.ones(len(TestGoodSamples));TestGoodLabels.fill(-1); TestGoodLabels = TestGoodLabels.tolist()
    logger.info("All labels created")

    TrainSamples = TrainMalSamples + TrainGoodSamples
    TestSamples = TestMalSamples + TestGoodSamples
    TrainLabels = TrainMalLabels + TrainGoodLabels
    TestLabels = TestMalLabels + TestGoodLabels
    NumTestMalSamples = len(TestMalLabels)
    del MalSamples, GoodSamples
    logger.info ("All Samples loaded into training and testing sets")
    print "# Train Samples", len(TrainSamples)
    print "# Train Labels", len(TrainLabels)
    print "# Test Samples", len(TestSamples)
    print "# Test Labels", len(TestLabels)

    # step 2 - feature extracting
    TFIDFTransformer = TfidfTransformer()

    NewLineCVetorizer = CountVectorizer(input=u'filename',
                                                  lowercase=True,
                                                  token_pattern=None,
                                                  tokenizer=NewLineTokenizer,
                                                  dtype=np.float64)

    print 'performing count vectorizing'
    TrainDocsTermsFVs = NewLineCVetorizer.fit_transform(TrainSamples)
    TestDocsTermsFVs = NewLineCVetorizer.transform(TestSamples)
    print 'performing tf-idf vectorizing'
    TrainFVs = TFIDFTransformer.fit_transform(TrainDocsTermsFVs)
    TestFVs = TFIDFTransformer.transform(TestDocsTermsFVs)


    print 'train term-doc matrix: ', TrainFVs.shape #rowsx cols, rows = docs, cols = features/terms
    print 'test term-doc matrix: ', TestFVs.shape

    # step 3 - classification
    logger.info("Performing Cross Validation")
    BestModel_SVM.fit(TrainDocsTermsFVs, TrainLabels)
    BestModel_PA.fit(TrainDocsTermsFVs, TrainLabels)
    BestModel_Perceptron.fit(TrainDocsTermsFVs, TrainLabels)
    BestModel_SGD.fit(TrainDocsTermsFVs, TrainLabels)
    print 'best model', BestModel_SVM.best_estimator_, BestModel_SVM.best_score_
    print 'best model', BestModel_PA.best_estimator_, BestModel_PA.best_score_
    print 'best model', BestModel_Perceptron.best_estimator_, BestModel_Perceptron.best_score_
    print 'best model', BestModel_SGD.best_estimator_, BestModel_SGD.best_score_

    logger.info("Applying Best Model on Testing Set")
    modeldict = {BestModel_SVM.best_estimator_:'SVM', BestModel_PA.best_estimator_:'PA', BestModel_Perceptron.best_estimator_:'Perceptron', BestModel_SGD.best_estimator_:'SGD'}
    for Model in [BestModel_SVM.best_estimator_, BestModel_PA.best_estimator_, BestModel_Perceptron.best_estimator_, BestModel_SGD.best_estimator_]:
        T0 = time()
        f = open(modeldict[Model]+'_'+Type+'.txt', 'w')
        f1 = open(modeldict[Model]+'_'+Type+'_Metadata.txt', 'w')
        try:
            Model.partial_fit(TrainFVs, TrainLabels, classes=[-1, +1])
        except:
            Model.fit(TrainFVs, TrainLabels)
        PredictedLabels = []
        NewTestLabels = []
        i = 0
        for TestFV, TestLabel in zip(TestFVs, TestLabels):
            #Mal Sample
            if i < NumTestMalSamples:
                TestMalLabel = np.array([TestLabel])
                PredictedLabel = Model.predict(TestFV)
                PredictedLabels.append(float(PredictedLabel))
                NewTestLabels.append(TestLabel)
                if float(PredictedLabel) != TestLabel:
                    try:
                        Model.partial_fit(TestFV, np.array([TestLabel]), classes=[-1, +1])#update the model
                        logger.info("Model Partially Fitted")
                    except:
                        logger.error("Partially Fitted Failed")
                        pass
                PredictedMalLabel = np.array([float(PredictedLabel)])
                print >>f1, (metrics.classification_report(TestMalLabel,PredictedMalLabel, target_names=['Sample', 'Sample']))
                print >>f1, "Zero-one classification loss:", metrics.zero_one_loss(TestMalLabel, PredictedMalLabel)
                print >>f1, '-'*100
            #Ben Sample
            if NumTestMalSamples+i < len(TestLabels):
                TestLabel = TestLabels[NumTestMalSamples+i]
                TestFV = TestFVs[NumTestMalSamples+i]
                TestGoodLabel = np.array([TestLabel])
                PredictedLabel2 = Model.predict(TestFV)
                PredictedLabels.append(float(PredictedLabel2))
                NewTestLabels.append(TestLabel)
                if float(PredictedLabel2) != TestLabel:
                    try:
                        Model.partial_fit(TestFVs[NumTestMalSamples+i], np.array([TestLabel]), classes=[-1, +1])#update the model
                        logger.info("Model Partially Fitted")
                    except:
                        logger.error("Partially Fitted Failed")
                        pass
                PredictedGoodLabel = np.array([float(PredictedLabel2)])
                print >>f1, (metrics.classification_report(TestGoodLabel,PredictedGoodLabel, target_names=['Sample', 'Sample']))
                print >>f1, "Zero-one classification loss:", metrics.zero_one_loss(TestGoodLabel, PredictedGoodLabel)
                print >>f1, '-'*100
            i += 1


        print >>f, 'Best model', Model
        print >>f, '-'*100
        print >>f, '-'*43+'Whole Database'+'-'*43
        Accuracy = metrics.accuracy_score(PredictedLabels, NewTestLabels)
        print >>f, "Test Set Accuracy = ", Accuracy
        print >>f, 'testing time', time() - T0
        print >>f, (metrics.classification_report(NewTestLabels,
                PredictedLabels, target_names=['Goodware', 'Malware']))    # raw_input()

        print >>f, 'Classifier Top Features'
        print >>f, '-'*100
        Vocab = NewLineCVetorizer.get_feature_names()
        FeautureImportances = Model.coef_[0]
        TopFeatureIndices = FeautureImportances.argsort()[-100:][::-1]
        for FIndex in TopFeatureIndices:
            print >>f, Vocab[FIndex], FeautureImportances[FIndex]
        print >>f, '-'*100

        print >>f, 'before deleting rows TestFVs.shape', TestFVs.shape
        for i in xrange(len(TestSamples)):
            if -1 == TestLabels[i]:
                TestFVss= TestFVs[:i, :]
                break
        print >>f, 'after deleting rows TestFVs.shape', TestFVss.shape

        FeatureImportancesSparseArray = ssp.lil_matrix((TestFVss.shape[1],TestFVss.shape[1]))
        FeatureImportancesSparseArray.setdiag(FeautureImportances)
        AllFVsTimesW = TestFVss*FeatureImportancesSparseArray

        print >>f, '-'*100
        AvgFV = AllFVsTimesW.mean(axis=0)
        AvgFV = AvgFV.view(dtype=np.float64).reshape(AvgFV.shape[1],-1)
        AvgFV = np.array(AvgFV).reshape(-1,)
        TopRes = AvgFV.argsort()[-100:][::-1]
        print >>f, 'Top Feats of Test Positive Vector * Feature Importance Vector'
        for Sindex in TopRes:
            print >>f, Vocab[Sindex], AvgFV[Sindex]
        print >>f, '-'*100

def main(MalwareDirName,
         GoodwareDirName,
         TestSplit,
         Extn):

    Classification(MalwareDirName,
                   GoodwareDirName,
                   TestSplit,
                   Extn)

if __name__ == "__main__":
     main(MalwareDirName=sys.argv[1],
          GoodwareDirName=sys.argv[2],
          TestSplit=float(sys.argv[3]),
          Extn=sys.argv[4])
