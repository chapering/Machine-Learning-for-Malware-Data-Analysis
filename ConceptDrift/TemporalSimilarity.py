import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
import os
import sys
import logging
from scipy.spatial.distance import cosine
from sklearn.metrics.pairwise import linear_kernel
import random

import matplotlib  
matplotlib.use('Agg') 

import matplotlib.pyplot as plt
from numpy import ma
from matplotlib import colors, ticker, cm
from matplotlib.mlab import bivariate_normal
from statsmodels.sandbox.regression.predstd import wls_prediction_std
from scipy import stats
import statsmodels.api as sm


#logging level
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger('sys.stdout')


def GetFilesWithExtn (RootDir, Familyname, Extn):
    '''
    lllr to "find" command. List all the files from the RootDir with a given Extn
    :param RootDir: root to find files from
    :param Extn: extension to look for
    :return: sorted list of files with a given extension from the root dir and its child dirs
    '''

    #FilesToProcess = [os.path.join (RootDir, File) for File in os.listdir (RootDir) if File.endswith (Extn)]
    FilesToProcess = []
    for Root,Folders,Files in os.walk(RootDir):
        for F in Files:
            if F.endswith(Extn) and (Familyname in F):
                FilesToProcess.append(os.path.join(Root,F))

    FilesToProcess = list(set(FilesToProcess))
    FilesToProcess.sort()
    return FilesToProcess


def NewLineTokenizer (Str):
    return Str.split('\n')

def NewLineTokenizerNoContext (Str):
    StrList = Str.split('\n')
    for Index,Str in enumerate(StrList):
        if '~' in Str:
            StrList[Index] = Str.split('~')[1]
    return StrList

def ReadVocab (VocabFName):
    try:
        print 'trying to load vocab from', VocabFName
        Vocab = [l.strip() for l in open (VocabFName,'r').xreadlines()]
        return Vocab
    except:
        print 'could not load vocab from {} exiting... '.format(VocabFName)
        exit(0)

def Classification(MalwareCorpus,
                   Familyname,
                   Extn):

    # step 1: split all samples to training set and test set (3:1)

    MalSamples = GetFilesWithExtn(MalwareCorpus, Familyname, Extn)
    MalSamples.sort()

    logger.info ("All Samples loaded")
    print '# mal samples:', len(MalSamples)

    #step 2 - feature extracting
    
    if 'datatxt' in Extn:
        Type = 'Drebin'
    elif 'WL2' in Extn:
        Type = 'WLK'
    elif '.txt' in Extn:
        Type = 'CSBD'

    try:
        Vocab = ReadVocab(Familyname+'_'+Type+'.txt')
    except:
        print 'could not load vocab, exiting... '
        exit(0)


    TFIDFTransformer = TfidfTransformer()
    NewLineCVectorizer = CountVectorizer(input=u'filename',
                                                  lowercase=True,
                                                  token_pattern=None,
                                                  tokenizer=NewLineTokenizer,
                                                  #binary=True,
                                                  vocabulary=Vocab,
                                                  dtype=np.float64)

    print 'performing count vectorizing'
    DocsTermsFV = NewLineCVectorizer.fit_transform(MalSamples)
    print DocsTermsFV.toarray()
    '''
    for sample in MalSamples:
        print NewLineCVectorizer.fit(sample)
        DocsTermsFVs.append(NewLineCVectorizer.fit(sample))
    '''

    print 'performing tf-idf vectorizing'
    FV = TFIDFTransformer.fit_transform(DocsTermsFV)
    print 'train term-doc matrix: ', FV.shape #rowsx cols, rows = docs, cols = features/terms
    #print FV.toarray()
    distance = []
    '''
    if BaseSample != 1:
        BaseSample = random.randint(0, len(MalSamples)/10)
    '''
    #j = BaseSample
    MeanSample = FV.mean(axis=0)
    #MalSamples = MalSamples[BaseSample:] 
    print 'The base sample is mean sample'
    for i, vector in enumerate(FV):
        if i < len(MalSamples):
            distance.append( 1 - cosine(FV.toarray()[i], MeanSample))
        else: break
    '''
    for sample in MalSamples:
        print os.path.basename(sample)
    print distance
    '''
    #SimMatric = linear_kernel(FV, FV)
    #print SimMatric
    
    x = np.linspace(1, len(MalSamples), len(MalSamples))
    y = np.array(distance)


    
    x = sm.add_constant(x)
    
    model = sm.OLS(y, x)
    fitted = model.fit()
    x_pred = np.linspace(x.min(), x.max(), len(MalSamples))
    print x_pred.shape
    print x.shape
    print y.shape
    x_pred2 = sm.add_constant(x_pred)
    y_pred = fitted.predict(x_pred2)

    y_hat = fitted.predict(x)
    y_err = y - y_hat
    mean_x = x.T[1].mean()
    n =len(x)
    dof = n - fitted.df_model -1
    t = stats.t.ppf(1-0.025, df = dof)
    s_err = np.sum(np.power(y_err, 2))
    conf = t * np.sqrt((s_err/(n-2)) * (1.0/n +(np.power((x_pred-mean_x), 2) / ((np.sum(np.power(x_pred, 2))) - n*(np.power(mean_x, 2))))))
    upper = y_pred + abs(conf)
    lower = y_pred - abs(conf)
    sdev, lower, upper = wls_prediction_std(fitted, exog=x_pred2, alpha=0.05)
    
    fig, ax = plt.subplots()
    ax.scatter(np.linspace(1, len(MalSamples), len(MalSamples)), y, facecolor='black')
    
    ax.plot(x_pred, y_pred, '-', color='darkorchid', linewidth=2)
    ax.fill_between(x_pred, lower,upper, alpha=0.1)
    
    plt.ylim(0,1.05)
    plt.xlim(0,len(MalSamples))
    if 'WL2' in Extn:
        plt.savefig(Familyname+'_MeanSimilaity_WLK.jpg',bbox_inches='tight', dpi=600)
    elif 'datatxt' in Extn:
        plt.savefig(Familyname+'_MeanSimilaity_Drebin.jpg',bbox_inches='tight', dpi=600)
    elif 'Feature' in Extn:
        plt.savefig(Familyname+'_MeanSimilaity_CSBD.jpg',bbox_inches='tight', dpi=600)
    else :    plt.savefig(Familyname+'.jpg',bbox_inches='tight', dpi=600)
    plt.clf()
    print 'fig saved'




def main(MalwareDirName,
         Familyname,
         Extn='Features'):


    Classification(MalwareDirName,
                   Familyname,
                   Extn)

if __name__ == "__main__":

     main(MalwareDirName=sys.argv[1],
          Familyname=sys.argv[2],
          Extn=sys.argv[3])  



